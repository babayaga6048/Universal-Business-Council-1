APE-Bench I is a new benchmark designed to test how well AI models can handle real-world tasks in formal mathematics. Unlike traditional benchmarks that focus on isolated theorem proving, APE-Bench I emphasizes the iterative and complex nature of maintaining large formal math libraries. It introduces the concept of Automated Proof Engineering (APE), aiming to automate tasks like adding features, refactoring proofs, and fixing bugs using AI. This benchmark is built from over 10,000 real-world commits in Mathlib4, providing a realistic and challenging environment for evaluating AI capabilities in proof engineering. https://www.blockchain-council.org/ai/ape-bench-i-set-to-test-automated-proof-engineering/
